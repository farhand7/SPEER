{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import logistic_regression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = pd.read_csv('../../../../../../data/train/train_all_g.csv', index_col=(0,1))\n",
    "e = pd.read_csv('../../../../../../data/train/train_all_tissues.csv', index_col=(0,1)).dropna(thresh = 3)\n",
    "train = pd.concat([g,e[\"median\"]], axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add discrete training labels\n",
    "train[\"labels\"] = sklearn.preprocessing.binarize(np.abs(train[\"median\"].values).reshape(-1,1), threshold = 1.5).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### processing data helper functions #####\n",
    "def processTissueGroups(tissue_groups_path):\n",
    "    tissue_groups = {}\n",
    "    f = open(tissue_groups_path)\n",
    "    for l in f:\n",
    "        w = l.strip().split(',')\n",
    "        group = w[0]\n",
    "        tissue_groups[group] = []\n",
    "        for tissue in w[1:]: tissue_groups[group].append(tissue)\n",
    "    return tissue_groups    \n",
    "    \n",
    "\n",
    "def generateTrainTest(train, annotation_columns):\n",
    "    '''\n",
    "        Training data contains annotation columns and other data columns\n",
    "        annotation_columns is a list of genomic annotations\n",
    "    '''\n",
    "    annotation_columns.insert(0, 'gene_id')\n",
    "    train.insert(0, 'gene_id', train.index.get_level_values('gene_id'))\n",
    "    train.index = train.index.get_level_values('subject_id')\n",
    "\n",
    "    # boolean mask - mark True for all duplicates and original\n",
    "    duplicates_bool = train.duplicated(subset = annotation_columns, keep = False)\n",
    "    # isolate training data w/ no duplicates - complement of boolean mask\n",
    "    train_nodups = train[~duplicates_bool]\n",
    "    train_nodups.index = [train_nodups.index, train_nodups['gene_id']]\n",
    "    train_nodups = train_nodups.drop('gene_id', axis=1)\n",
    "\n",
    "    # order duplicates consecutively\n",
    "    duplicates = train[duplicates_bool].sort_values(by = annotation_columns)\n",
    "    # remove odd duplicates\n",
    "    duplicates = duplicates.groupby(by = annotation_columns).filter(lambda x: len(x) % 2 == 0)\n",
    "    duplicates.index = [duplicates.index, duplicates['gene_id']]\n",
    "    duplicates = duplicates.drop('gene_id', axis=1)\n",
    "    n1 = duplicates.iloc[::2]\n",
    "    n2 = duplicates.iloc[1::2]\n",
    "    return train_nodups, n1, n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed all data...\n",
      "processed  group1  tissues.\n",
      "processed  brain  tissues.\n"
     ]
    }
   ],
   "source": [
    "# split train/test and create relevant matrices\n",
    "expression_path = '../../../../../../data/train/train_all_tissues.csv'\n",
    "annotations_path = '../../../../../../data/train/train_all_g.csv'\n",
    "tissue_groups_path = '../tissue_groups/tissue_groups.v7.txt'\n",
    "\n",
    "train_list, test_list = [], []\n",
    "tissues = []\n",
    "annotations = pd.read_csv(annotations_path, index_col=(0,1))\n",
    "expression = pd.read_csv(expression_path, index_col=(0,1)) \n",
    "\n",
    "tissue_groups = processTissueGroups(tissue_groups_path)\n",
    "for k,v in tissue_groups.items():\n",
    "    tissues.extend(v)\n",
    "annot_cols_original = list(annotations.columns)\n",
    "annot_cols_original.insert(0, 'intercept')\n",
    "# scale annotations and add intercept\n",
    "annotations = annotations / (annotations.max() - annotations.min())\n",
    "annotation_columns = list(annotations.columns)\n",
    "\n",
    "print (\"processed all data...\")\n",
    "\n",
    "#genomeonly_sharedtissue_beta = trainSharedGenomeOnlyModel(annotations, expression)\n",
    "\n",
    "for group in tissue_groups:\n",
    "    # identify tissue-specific expression data\n",
    "    expr_group = expression[tissue_groups[group]]\n",
    "    # first, limit to samples you want and take median\n",
    "    if len(expr_group.columns) == 2:\n",
    "        expr_group = expr_group.dropna()\n",
    "    elif len(expr_group.columns) == 3:\n",
    "        expr_group = expr_group.dropna(thresh = 2)\n",
    "    elif len(expr_group.columns) == 4:\n",
    "        expr_group = expr_group.dropna(thresh = 3)\n",
    "    else:\n",
    "        expr_group = expr_group.dropna(thresh = 3)\n",
    "\n",
    "    # compute med(abs(z-score)) for each sample\n",
    "    expr_group[\"expr_median\"] = np.abs(expr_group).median(axis=1)\n",
    "    # concatenate annotations with expression data\n",
    "    train = pd.concat([annotations, expr_group[\"expr_median\"]], axis = 1)\n",
    "    # drop samples with any missing annotations\n",
    "    train = train.dropna()\n",
    "\n",
    "    # add binarized expression label\n",
    "    train[\"expr_label\"] = sklearn.preprocessing.binarize(np.abs(train[\"expr_median\"]).reshape(-1,1), threshold = 1.5)\n",
    "    # add posterior\n",
    "    train[\"posterior\"] = 0\n",
    "    train[\"tissue\"] = str(group)\n",
    "\n",
    "    train, n1, n2 = generateTrainTest(train, annotation_columns)\n",
    "    # add intercept\n",
    "    train.insert(0, 'intercept', 1)\n",
    "    n1.insert(0, 'intercept', 1)\n",
    "    n2.insert(0, 'intercept', 1)\n",
    "\n",
    "    train_list.append(train)\n",
    "    test_list.append([n1, n2])\n",
    "\n",
    "    print (\"processed \", group, \" tissues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_resample(X, n=None):\n",
    "    \"\"\" \n",
    "    citation: http://nbviewer.jupyter.org/gist/aflaxman/6871948\n",
    "    Bootstrap resample an array_like\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "      data to resample\n",
    "    n : int, optional\n",
    "      length of resampled array, equal to len(X) if n==None\n",
    "    Results\n",
    "    -------\n",
    "    returns X_resamples\n",
    "    \"\"\"\n",
    "    if n == None:\n",
    "        n = len(X)\n",
    "        \n",
    "    resample_i = np.floor(np.random.rand(n)*len(X)).astype(int)\n",
    "    X_resample = X.iloc[resample_i]\n",
    "    return X_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimateBetaParent(beta_children, lambda_hp_children, lambda_hp_parent, num_tissues):\n",
    "    '''\n",
    "        Estimate beta parent \n",
    "        beta_j = (2 * \\sum_c lambda^c * beta_j^c) / (2*lamda + L * \\sum_c lambda^c)\n",
    "    '''\n",
    "\n",
    "    return (np.sum((np.array([lambda_hp_children]).T * beta_children), axis = 0)) / (lambda_hp_parent + np.sum(lambda_hp_children))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "def _cross_validate(g, expr_label, beta_init, beta_parent_init, lambda_set):\n",
    "    '''\n",
    "        Cross-validate beta MAP estimation to find optimal lambda\n",
    "    '''\n",
    "    X = g\n",
    "    Y = expr_label\n",
    "    K = 5\n",
    "    scores_list = np.zeros((len(lambda_set), K))\n",
    "    for k in range(K):\n",
    "        training = np.array([x for i, x in enumerate(X) if i % K != k])\n",
    "        training_labels = np.array([x for i, x in enumerate(Y) if i % K != k])\n",
    "        validation = np.array([[x for i, x in enumerate(X) if i % K == k]])\n",
    "        validation_labels = np.array([x for i, x in enumerate(Y) if i % K == k])\n",
    "        for i in range(len(lambda_set)):\n",
    "            beta = lr.sgd(training, training_labels, beta_init, beta_parent_init, float(lambda_set[i]))\n",
    "            scores = lr.log_prob(validation, beta).reshape(-1)\n",
    "            auc = sklearn.metrics.roc_auc_score(validation_labels, scores)\n",
    "            print(lambda_set[i], auc)\n",
    "            scores_list[i][k] = auc\n",
    "    # average across all folds for each lambda\n",
    "    lambda_averages = np.mean(scores_list, axis=1)\n",
    "    print(lambda_averages)\n",
    "    # sanity check\n",
    "    assert len(lambda_averages) == len(lambda_set)\n",
    "    optimal_lambda = lambda_set[np.argmax(lambda_averages)]\n",
    "    return optimal_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 1.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tissue:  0\n",
      "1e-06 0.537928763419\n",
      "1e-05 0.537929163006\n",
      "0.0001 0.537929148206\n",
      "0.001 0.537928600624\n",
      "0.01 0.537914156284\n",
      "0.1 0.537825477512\n",
      "1.0 0.538042527391\n",
      "10.0 0.540775097462\n",
      "100.0 0.544210600718\n",
      "1000.0 0.543474856978\n",
      "10000.0 0.541306903705\n",
      "100000.0 0.54093666391\n",
      "1000000.0 0.540892324524\n",
      "1e-06 0.560080261152\n",
      "1e-05 0.560079420238\n",
      "0.0001 0.560080444624\n",
      "0.001 0.560084603328\n",
      "0.01 0.560110518781\n",
      "0.1 0.560369275789\n",
      "1.0 0.561830846184\n",
      "10.0 0.562613538734\n",
      "100.0 0.555219408941\n",
      "1000.0 0.544744138184\n",
      "10000.0 0.54055337244\n",
      "100000.0 0.539894141414\n",
      "1000000.0 0.539827739754\n",
      "1e-06 0.546070724847\n",
      "1e-05 0.546070789535\n",
      "0.0001 0.546070999771\n",
      "0.001 0.54606985156\n",
      "0.01 0.546056283258\n",
      "0.1 0.545885054197\n",
      "1.0 0.54498474318\n",
      "10.0 0.543324105918\n",
      "100.0 0.541570463524\n",
      "1000.0 0.537215022397\n",
      "10000.0 0.53479617742\n",
      "100000.0 0.534620306997\n",
      "1000000.0 0.534604442272\n",
      "1e-06 0.539439768946\n",
      "1e-05 0.539440245528\n",
      "0.0001 0.539439942249\n",
      "0.001 0.53943842585\n",
      "0.01 0.539456478216\n",
      "0.1 0.539541945336\n",
      "1.0 0.540066850366\n",
      "10.0 0.540103633866\n",
      "100.0 0.53338346018\n",
      "1000.0 0.52466968431\n",
      "10000.0 0.522497002657\n",
      "100000.0 0.52219645243\n",
      "1000000.0 0.522168550693\n",
      "1e-06 0.548582620993\n",
      "1e-05 0.548582469712\n",
      "0.0001 0.548582742019\n",
      "0.001 0.548581350228\n",
      "0.01 0.548569066157\n",
      "0.1 0.548478796393\n",
      "1.0 0.548382853568\n",
      "10.0 0.54846934129\n",
      "100.0 0.544905856518\n",
      "1000.0 0.535562943546\n",
      "10000.0 0.530535233891\n",
      "100000.0 0.529745952111\n",
      "1000000.0 0.529665908988\n",
      "[ 0.54642043  0.54642042  0.54642066  0.54642057  0.5464213   0.54642011\n",
      "  0.54666156  0.54705714  0.54385796  0.53713333  0.53393774  0.5334787\n",
      "  0.53343179]\n",
      "tissue:  1\n",
      "1e-06 0.563429478757\n",
      "1e-05 0.563429254171\n",
      "0.0001 0.56343239838\n",
      "0.001 0.563481582795\n",
      "0.01 0.563716500136\n",
      "0.1 0.564600247494\n",
      "1.0 0.563564455165\n",
      "10.0 0.555181768977\n",
      "100.0 0.543444885382\n",
      "1000.0 0.535707323537\n",
      "10000.0 0.53370917862\n",
      "100000.0 0.533463818013\n",
      "1000000.0 0.53345101659\n",
      "1e-06 0.565985601238\n",
      "1e-05 0.565984528593\n",
      "0.0001 0.565984743122\n",
      "0.001 0.566004908854\n",
      "0.01 0.566016600688\n",
      "0.1 0.565902578489\n",
      "1.0 0.565560190098\n",
      "10.0 0.56203783735\n",
      "100.0 0.552580967024\n",
      "1000.0 0.540227739767\n",
      "10000.0 0.535380348236\n",
      "100000.0 0.534780954024\n",
      "1000000.0 0.534693426164\n",
      "1e-06 0.573457903398\n",
      "1e-05 0.573457903398\n",
      "0.0001 0.573464665449\n",
      "0.001 0.573482661229\n",
      "0.01 0.573713770677\n",
      "0.1 0.574657840227\n",
      "1.0 0.575389777699\n",
      "10.0 0.570894649865\n",
      "100.0 0.553324660458\n",
      "1000.0 0.539507063944\n",
      "10000.0 0.536332499202\n",
      "100000.0 0.535874097593\n",
      "1000000.0 0.535826436041\n",
      "1e-06 0.532921198378\n",
      "1e-05 0.532920997981\n",
      "0.0001 0.532922400755\n",
      "0.001 0.532910777772\n",
      "0.01 0.532803465573\n",
      "0.1 0.532096768149\n",
      "1.0 0.532028433023\n",
      "10.0 0.530585780322\n",
      "100.0 0.521473161029\n",
      "1000.0 0.518148787412\n",
      "10000.0 0.518121232926\n",
      "100000.0 0.518027948465\n",
      "1000000.0 0.518006706461\n",
      "1e-06 0.536326454731\n",
      "1e-05 0.536326565391\n",
      "0.0001 0.536336856736\n",
      "0.001 0.536406572296\n",
      "0.01 0.536950796307\n",
      "0.1 0.539821528165\n",
      "1.0 0.542157110102\n",
      "10.0 0.540013190627\n",
      "100.0 0.533312308006\n",
      "1000.0 0.531446144176\n",
      "10000.0 0.530459503006\n",
      "100000.0 0.530403177259\n",
      "1000000.0 0.530382815889\n",
      "[ 0.55442413  0.55442385  0.55442821  0.5544573   0.55464023  0.55541579\n",
      "  0.55573999  0.55174265  0.5408272   0.53300741  0.53080055  0.53051\n",
      "  0.53047208]\n",
      "[10.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "num_tissues = len(train_list)\n",
    "# beta is a T x M matrix, where T = # of tissues and M = number of features (not including intercept)\n",
    "beta = np.zeros((K, num_tissues, len(annot_cols_original) - 1))\n",
    "beta_parent = np.zeros(len(annot_cols_original) - 1)\n",
    "\n",
    "delta = np.zeros((K, num_tissues, len(annot_cols_original) - 1))\n",
    "delta_parent = np.zeros((K, len(annot_cols_original) - 1))\n",
    "lambda_set = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n",
    "optimal_lambdas = [1, 1]\n",
    "# determine optimal lambdas on one simulated data set\n",
    "for j in range(num_tissues):\n",
    "    print(\"tissue: \", j)\n",
    "    train_sample = bootstrap_resample(train_list[j])\n",
    "    g = train_sample[annot_cols_original].values\n",
    "    expr_label = train_sample[\"expr_label\"].values\n",
    "    optimal_lambda = _cross_validate(g, expr_label, np.zeros(len(annot_cols_original)), np.zeros(len(annot_cols_original)), lambda_set)\n",
    "    optimal_lambdas[j] = optimal_lambda\n",
    "print(optimal_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 100\n",
    "num_tissues = len(train_list)\n",
    "# beta is a T x M matrix, where T = # of tissues and M = number of features (not including intercept)\n",
    "beta = np.zeros((K, num_tissues, len(annot_cols_original) - 1))\n",
    "beta_parent = np.zeros(len(annot_cols_original) - 1)\n",
    "\n",
    "delta = np.zeros((K, num_tissues, len(annot_cols_original) - 1))\n",
    "delta_parent = np.zeros((K, len(annot_cols_original) - 1))\n",
    "optimal_lambdas = [1, 1]\n",
    "lambda_set = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n",
    "\n",
    "# for each tissue\n",
    "for j in range(num_tissues):\n",
    "    # generate K random data sets\n",
    "    for i in range(K):\n",
    "        train_sample = bootstrap_resample(train_list[j])\n",
    "        g = train_sample[annot_cols_original]\n",
    "        expr_label = train_sample[\"expr_label\"]\n",
    "        optimal_lambda = _cross_validate(g, expr_label, np.zeros(len(annot_cols_original)), np.zeros(len(annot_cols_original)), lambda_set)\n",
    "        # compute L2 regularized logistic regression and store non-intercept terms\n",
    "        beta[i][j] = lr.sgd(g.values, expr_label.values, np.zeros(len(annot_cols_original)), np.zeros(len(annot_cols_original)), optimal_lambda)[1:]\n",
    "# for each dataset\n",
    "for i in range(K):\n",
    "    beta_parent = estimateBetaParent(beta[i], np.ones(num_tissues), 1, num_tissues)\n",
    "    # estimate variance between each beta child and beta parent for this trial and variance of parent\n",
    "    for j in range(num_tissues):\n",
    "        delta[i][j] = (beta[i][j] - beta_parent)\n",
    "    delta_parent[i] = beta_parent\n",
    "    \n",
    "    if i > 2:\n",
    "        lambda_hp = computeEmpiricalVariance(delta, i+1)\n",
    "        # simplifying assumption - variance is the smae across all features, so we take the average of the feature variances\n",
    "        lambda_hp = np.sum(lambda_hp, axis=1) / lambda_hp.shape[1]\n",
    "\n",
    "        lambda_hp_parent = computeEmpiricalVarianceParent(delta_parent, i+1)\n",
    "        lambda_hp_parent = np.sum(lambda_hp_parent) / lambda_hp_parent.shape[0]\n",
    "        \n",
    "        print(\"lambda inverse: \", lambda_hp)\n",
    "        print(\"lambda parent: \", lambda_hp_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeEmpiricalVariance(delta, K):\n",
    "    lambda_hp = np.zeros((num_tissues, len(annot_cols_original) - 1))\n",
    "    for t in range(num_tissues):\n",
    "        for j in range(len(annot_cols_original) - 1):\n",
    "            lambda_hp[t][j] = np.sum(delta[:,t,j]**2) / (K-1)\n",
    "    return lambda_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeEmpiricalVarianceParent(delta, K):\n",
    "    lambda_hp = np.zeros(len(annot_cols_original) - 1)\n",
    "    for j in range(len(annot_cols_original) - 1):\n",
    "        lambda_hp[j] = np.sum(delta[:,j]**2) / (K-1)\n",
    "    return lambda_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
